version: "3.9"
name: haichat

services:
  api-online:
    build:
      context: ./services/api-online
      dockerfile: Dockerfile
    environment:
      MISTRAL_API_KEY: ${MISTRAL_API_KEY}
      MISTRAL_MODEL: ${MISTRAL_MODEL}
      APP_CORS_ORIGINS: ${CORS_ORIGINS}
      APP_PORT: ${PORT_API_ONLINE}
    ports:
      - "${PORT_API_ONLINE}:${PORT_API_ONLINE}"
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:${PORT_API_ONLINE}/ping"]
      interval: 10s
      timeout: 3s
      retries: 10

  api-offline-gpu:
    profiles: ["gpu"]
    image: vllm/vllm-openai:latest
    command:
      [
        "--model",
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "--gpu-memory-utilization",
        "0.7",
        "--api-key",
        "testingvllm",
        "--host",
        "0.0.0.0"
      ]
    ports:
      - "${PORT_VLLM:-8000}:8000"
    gpus: all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - VLLM_DOWNLOAD_DIR=/models
    volumes:
      - vllm-cache:/models
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8000/v1/models"]
      interval: 20s
      timeout: 5s
      retries: 20


  
  api-offline-cpu:
    profiles: ["cpu"]
    image: vllm-cpu-noavx512
    command:
      [
        "--model",
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "--dtype",
        "bfloat16",
        "--api-key",
        "testingvllm",
        "--host",
        "0.0.0.0"
      ]
    ports:
      - "${PORT_VLLM:-8000}:8000"
    environment:
      - VLLM_DOWNLOAD_DIR=/models
    volumes:
      - vllm-cache:/models



  chat:
    build:
      context: ./apps/chat
      dockerfile: Dockerfile
      args:
        NEXT_PUBLIC_ONLINE_ENDPOINT: ${NEXT_PUBLIC_ONLINE_ENDPOINT}
        NEXT_PUBLIC_VLLM_CHAT: ${NEXT_PUBLIC_VLLM_CHAT}
        NEXT_PUBLIC_VLLM_MODEL: ${NEXT_PUBLIC_VLLM_MODEL}
        NEXT_PUBLIC_OFFLINE_API_KEY: ${NEXT_PUBLIC_OFFLINE_API_KEY}
    environment:
      PORT: ${PORT_CHAT:-3000}
      NODE_ENV: production
    ports:
      - "${PORT_CHAT:-3000}:${PORT_CHAT:-3000}"
    depends_on:
      - api-online
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:${PORT:-3000} >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 20

volumes:
  vllm-cache: {}
